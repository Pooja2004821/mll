{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2MlPsXlRt47"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is a parameter?\n",
        "- A parameter is a quantity or value that describes a characteristic of a population in statistics or a function or method in programming. Here's a breakdown depending on the context:\n",
        "- In Statistics : A parameter is a numerical value that summarizes or describes an aspect of a population.It is fixed but usually unknown, because it's hard to collect data from the entire population.\n",
        "  - Common parameters include : Mean (μ), Proportion (p), Standard deviation (σ)\n",
        "  - Example : If you want to know the average height of all adults in India (population), the actual average height is the parameter.\n",
        "- In Mathematics : A parameter is a variable that defines a family of functions or equations. It’s often used to adjust the output without changing the form of the function.\n",
        "  - Example : In the equation y=mx+c, both m and c are parameters — they control the slope and intercept of the line.\n",
        "- In Programming (like Python) : A parameter is a variable used to pass information into a function.\n",
        "\n",
        "Q2.What is correlation? What does negative correlation mean?\n",
        "- Correlation is a statistical measure that describes the relationship between two variables — how they move together.\n",
        "- It tells us :\n",
        "    - Direction of the relationship (positive or negative)\n",
        "    - Strength of the relationship (weak, moderate, or strong)\n",
        "- Correlation Coefficient (r) : Ranges between -1 and +1.Calculated using Pearson’s formula in most cases.\n",
        "- What is Negative Correlation? : A negative correlation means that as one variable increases, the other decreases.\n",
        "    - Example : As exercise time increases, body fat decreases, As speed increases, travel time decreases\n",
        "    - This shows an inverse relationship.\n",
        "\n",
        "Q3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "- Machine Learning (ML) is a branch of Artificial Intelligence (AI) that focuses on building systems that can learn from data and improve automatically without being explicitly programmed.\n",
        "- Definition : Machine Learning is the study of algorithms that enable computers to learn from and make predictions or decisions based on data.\n",
        "- Main Components of Machine Learning\n",
        "Machine Learning involves several key components. Here are the most important ones:\n",
        "1. Data : The foundation of any ML model.Can be structured (tables) or unstructured (text, images).\n",
        "  - Split into : Training data (used to train the model), Test data (used to evaluate the model)\n",
        "  - Example : Heights and weights of people to predict BMI.\n",
        "2. Features (Input Variables) : Independent variables or characteristics used to make predictions.Also called predictors or attributes.\n",
        "  - Example : In a housing price prediction model: Size, Location, and Number of rooms are features.\n",
        "3. Model (Algorithm) : The mathematical structure or algorithm that finds patterns in the data.\n",
        "  - Examples : Linear Regression, Decision Trees, Neural Networks\n",
        "4. Training : The process of feeding data to the model so it can learn patterns.\n",
        "The model adjusts its internal parameters (like weights in neural networks) based on input-output examples.\n",
        "5. Prediction / Inference : After training, the model can predict outcomes for new, unseen data.\n",
        "   - Example: Predicting tomorrow’s temperature using past data.\n",
        "6. Evaluation : Checking how well the model performs using metrics like :\n",
        "                - Accuracy\n",
        "                - Precision\n",
        "                - Recall\n",
        "                - Mean Squared Error (MSE)\n",
        "\n",
        "7. Loss Function : A function that measures the error between the predicted output and the actual output.The model tries to minimize this during training.\n",
        "8. Optimization Algorithm :\n",
        "Techniques like Gradient Descent are used to reduce the loss function by adjusting the model’s internal parameters.\n",
        "\n",
        "Q4.How does loss value help in determining whether the model is good or not?\n",
        "- The loss value is a key metric in Machine Learning that measures how far off the model’s predictions are from the actual target values.\n",
        "- What is a Loss Value? : It is a numerical value output by a loss function, such as :\n",
        "            - Mean Squared Error (MSE) for regression\n",
        "            - Cross-Entropy Loss for classification\n",
        "            - A smaller loss value means the model is making better predictions.\n",
        "Why is Loss Important? :\n",
        "            - Direct Indicator of Model Accuracy:\n",
        "            - If the loss is low, your model's predictions are close to the actual output.\n",
        "            - If the loss is high, your model is making large errors.\n",
        "            - Guides the Learning Process:\n",
        "            - During training, the model adjusts its parameters to minimize the loss.This process is called optimization (usually with gradient descent).\n",
        "            - Helps in Model Comparison : You can compare models or configurations by looking at which one has the lower loss on a validation/test dataset.\n",
        "\n",
        "Q5. What are continuous and categorical variables?\n",
        "1. Continuous Variables : These are numerical variables that can take any value within a range — including fractions or decimals.\n",
        "- Characteristics :\n",
        "   - Quantitative (measurable)\n",
        "   - Can be counted or measured\n",
        "   - Values are infinite within a range\n",
        "- Examples :\n",
        "   - Height (e.g., 162.5 cm)\n",
        "   - Weight (e.g., 55.3 kg)\n",
        "   - Temperature (e.g., 37.2°C)\n",
        "   - Income, Speed, Age (in decimal)\n",
        "- You can perform arithmetic operations like mean, median, standard deviation on them.\n",
        "2. Categorical Variables : These are variables that represent categories or groups. They are qualitative in nature.\n",
        "- Characteristics :\n",
        "   - Values are labels or names, not numbers\n",
        "   - Can be nominal (no order) or ordinal (has order)\n",
        "- Examples :\n",
        "   - Type\tExample Values\n",
        "   - Nominal\tGender: Male, Female\n",
        "   - Color: Red, Green, Blue\n",
        "   - Ordinal\tEducation Level: High, Medium, Low\n",
        "   - Ratings: Good, Average, Poor\n",
        "- You can’t perform arithmetic on them (e.g., \"Red\" + \"Blue\" makes no sense).\n",
        "\n",
        "Q6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "-  How Do We Handle Categorical Variables in Machine Learning? : Most machine learning models cannot work with text or labels directly — they need numerical input. So, we must convert categorical variables into a form the model can understand.\n",
        "- Common Techniques to Handle Categorical Variables :\n",
        "1. Label Encoding : Converts categories to numbers.Each unique category is assigned an integer value.\n",
        "- Example :\n",
        "          - Gender: [\"Male\", \"Female\", \"Other\"]\n",
        "          - Encoded: [0, 1, 2]\n",
        "- Use when : The categories have no specific order Or when using tree-based models (e.g., Decision Trees, XGBoost)\n",
        "- Drawback : Adds fake ordinal meaning to labels for models like linear regression.\n",
        "2. One-Hot Encoding : Creates new binary columns for each category.1 if the category is present, 0 otherwise.\n",
        "- Example:\n",
        "         - Color: [\"Red\", \"Green\", \"Blue\"]\n",
        "         - One-hot:\n",
        "         - Red   Green  Blue\n",
        "         - 1      0      0\n",
        "         - 0      1      0\n",
        "- Best for : Nominal data (no order), Works well with most ML algorithms (Linear Regression, Neural Nets, etc.)\n",
        "3. Ordinal Encoding : Like label encoding, but assumes meaningful order between categories.\n",
        "- Example :\n",
        "         - Size: [\"Small\", \"Medium\", \"Large\"]\n",
        "         - Encoded: [1, 2, 3]\n",
        "- Use for : Ordinal categories (where order matters)\n",
        "4. Frequency / Count Encoding : Replaces each category with the number of times it appears in the dataset.\n",
        "- Example :\n",
        "         - City: [\"Delhi\", \"Mumbai\", \"Delhi\", \"Pune\"]\n",
        "         - Encoded: [2, 1, 2, 1]\n",
        "- Use for : High cardinality categories (lots of unique values), Works well for tree-based models\n",
        "5. Target / Mean Encoding : Replaces category with the mean target value for that category.\n",
        "- Example (House price prediction) :\n",
        "          - Neighborhood: A → Avg price = 50 Lakhs  \n",
        "                    -     B → Avg price = 70 Lakhs\n",
        "- Use for : High-cardinality variables, When there is a strong relationship between the category and the target\n",
        "\n",
        "\n",
        "Q7. What do you mean by training and testing a dataset?\n",
        "- In machine learning, we divide our dataset into two main parts: training and testing datasets. This helps us build a model and then evaluate how well it performs on unseen data.\n",
        "1. Training Dataset : The training set is the portion of data used to teach the model.The model learns patterns, relationships, and adjusts its internal parameters based on this data.\n",
        "- Example : If you're building a model to predict house prices, the training data includes:\n",
        "  - Features: size, location, number of rooms\n",
        "  - Labels: actual prices\n",
        "- The model uses this to learn how these features relate to the price.\n",
        "2. Testing Dataset : The test set is not shown to the model during training.It’s used to evaluate the model’s performance — i.e., how well it generalizes to new, unseen data.\n",
        "- Example : After training the model on 80% of your house price data, you use the remaining 20% to test whether it can correctly predict prices it hasn't seen before.\n",
        "\n",
        "Q8. What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module in Scikit-learn (sklearn) that provides tools to prepare or transform raw data before feeding it into a machine learning model.It’s used to clean, scale, encode, or normalize data — all critical steps in the data preprocessing phase of ML.\n",
        "- Why is Preprocessing Needed? :\n",
        "   - Most ML algorithms :\n",
        "         - Perform better when features are on the same scale\n",
        "         - Cannot work with non-numerical (categorical) data\n",
        "         - Need clean and normalized inputs\n",
        "\n",
        "Q9.What is a Test set?\n",
        "- A test set is a portion of your dataset that is not used during training, but is reserved for evaluating the performance of your machine learning model.\n",
        "- Purpose of the Test Set :\n",
        "   - To simulate real-world, unseen data\n",
        "   - To check if the model generalizes well (not just memorized training data)\n",
        "   - To measure accuracy, precision, recall, etc., after training\n",
        "- Why Is It Important? : When a model is trained, it learns patterns from the training data. But to be useful, it must also perform well on new, unseen data. The test set helps us check that.\n",
        "\n",
        "Q10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "- 1. How to Split Data for Model Fitting (Training & Testing) in Python?\n",
        "We typically use the train_test_split() function from Scikit-learn (sklearn).\n",
        "- Example in Python:\n",
        "     - from sklearn.model_selection import train_test_split\n",
        "     - #Suppose you have your features in X and labels in y\n",
        "     - #X = features, y = target\n",
        "     - X_train, X_test, y_train, y_test = train_test_split(\n",
        "          - X, y,                    # data\n",
        "          - test_size=0.2,           # 20% for testing\n",
        "          - random_state=42          # to make results reproducible\n",
        "     - )\n",
        "     - X_train, y_train: used to train the model\n",
        "     - X_test, y_test: used to evaluate/test the model\n",
        "2. How Do You Approach a Machine Learning Problem? : Here’s a structured and practical 7-step approach to solving any ML problem:\n",
        "- Step 1 : Understand the Problem\n",
        "    - What is the goal? (e.g., classification, regression)\n",
        "    - What does the dataset represent?\n",
        "    - What are the input features and target variable?\n",
        "- Step 2 : Collect and Explore the Data\n",
        "    - Load the data (CSV, Excel, database)\n",
        "    - Use pandas and matplotlib / seaborn to:\n",
        "    - View basic stats (df.describe(), df.info())\n",
        "    - Check for missing values, outliers, and distributions\n",
        "- Step 3 : Preprocess the Data\n",
        "    - Handle missing data (fillna(), dropna())\n",
        "    - Encode categorical variables (LabelEncoder, OneHotEncoder)  \n",
        "    - Scale/normalize features (StandardScaler, MinMaxScaler)\n",
        "    - Feature selection or engineering if needed\n",
        "- Step 4: Split the Data\n",
        "    - from sklearn.model_selection import train_test_split\n",
        "    - X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "- Step 5: Choose and Train a Model\n",
        "    - from sklearn.linear_model import LinearRegression\n",
        "    - model = LinearRegression()\n",
        "    - model.fit(X_train, y_train)\n",
        "Step 6: Evaluate the Model\n",
        "    - Use metrics based on your task:\n",
        "    - Task\tCommon Metrics\n",
        "    - Classification\tAccuracy, Precision, Recall, F1\n",
        "    - Regression\tMSE, MAE, R²\n",
        "         - from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "         - y_pred = model.predict(X_test)\n",
        "         - print(mean_squared_error(y_test, y_pred))\n",
        "- Step 7: Tune and Improve\n",
        "    - Try different models (SVM, Random Forest, etc.)\n",
        "    - Use cross-validation\n",
        "    - Perform hyperparameter tuning (GridSearchCV)\n",
        "    - Use more features or collect more data\n",
        "\n",
        "Q11. Why do we have to perform EDA before fitting a model to the data?\n",
        "- EDA is the process of analyzing and visualizing the data before using it to train a machine learning model.\n",
        "- It helps you understand:\n",
        "    - What your data looks like\n",
        "    - What problems your data may have\n",
        "    - How best to preprocess or model it\n",
        "- Reasons Why EDA is Essential :\n",
        "1. Understand the Data Structure\n",
        "   - What are the features and target?\n",
        "   - What data types are present? (numeric, categorical, dates, etc.)\n",
        "   - Are the features meaningful or redundant?\n",
        "2. Detect Missing or Corrupted Data\n",
        "   - EDA shows null values, empty strings, or anomalies.\n",
        "   - Helps you decide whether to:\n",
        "   - Drop missing rows/columns\n",
        "   - Fill missing values with mean/median/mode\n",
        "          - df.isnull().sum()\n",
        "3. Identify Data Imbalance\n",
        "   - In classification: Are there enough samples of each class?\n",
        "   - You might find 95% \"No\" and 5% \"Yes\" — which can lead to biased models.\n",
        "4. Detect Outliers and Unusual Values\n",
        "   - Some data points may be extreme or incorrect\n",
        "   - Outliers can skew model performance, especially for linear models\n",
        "5. Understand Feature Relationships\n",
        "   - How features are correlated with each other and the target\n",
        "   - Helps in:\n",
        "        - Feature selection\n",
        "        - Avoiding multicollinearity\n",
        "        - Understanding feature importance\n",
        "              - import seaborn as sns\n",
        "              - sns.heatmap(df.corr(), annot=True)\n",
        "\n",
        "\n",
        "Q12.What is correlation?\n",
        "- Correlation is a statistical measure that describes the strength and direction of a relationship between two numerical variables.It shows how one variable changes when another variable changes.The most common correlation measure is the Pearson correlation coefficient (r).\n",
        "\n",
        "Q13.What does negative correlation mean?\n",
        "- A negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "- Value of Correlation Coefficient (r):\n",
        "        - r < 0: Negative correlation\n",
        "        - r = –1: Perfect negative correlation\n",
        "        - r ≈ 0: Weak/no correlation\n",
        "        - r closer to –1: Stronger negative relationship\n",
        "- Graphically  : A scatter plot of two negatively correlated variables will slope downward from left to right.\n",
        "\n",
        "Q14.How can you find correlation between variables in Python?\n",
        "- To find the correlation between variables in Python, you typically use Pandas or NumPy, along with visualization libraries like Seaborn or Matplotlib for better insights.\n",
        "1. Using Pandas .corr() Method\n",
        "    - import pandas as pd\n",
        "    - #Sample dataset\n",
        "    - data = {\n",
        "        - 'X': [1, 2, 3, 4, 5],\n",
        "        - 'Y': [2, 4, 6, 8, 10],\n",
        "        - 'Z': [5, 3, 6, 2, 1]\n",
        "    - }\n",
        "    - df = pd.DataFrame(data)\n",
        "    - #Correlation matrix\n",
        "    - correlation_matrix = df.corr()\n",
        "    - print(correlation_matrix)\n",
        "    - .corr() calculates Pearson correlation by default.\n",
        "- Output is a matrix showing correlation between all pairs of variables.\n",
        "2. Using NumPy\n",
        "    - import numpy as np\n",
        "    - x = np.array([1, 2, 3, 4, 5])\n",
        "    - y = np.array([2, 4, 6, 8, 10])\n",
        "    - correlation = np.corrcoef(x, y)\n",
        "    - print(correlation)\n",
        "    - np.corrcoef() returns a correlation matrix.\n",
        "    - [0, 1] or [1, 0] index gives the correlation between x and y.\n",
        "3. Using Seaborn Heatmap (Visualization)\n",
        "    - import seaborn as sns\n",
        "    - import matplotlib.pyplot as plt\n",
        "    - #Heatmap\n",
        "    - sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "    - plt.show()\n",
        "- A heatmap makes it easy to visually inspect correlation values.\n",
        "4. Types of Correlation in Pandas\n",
        "    - df.corr(method='pearson')   # Default\n",
        "    - df.corr(method='kendall')\n",
        "    - df.corr(method='spearman')\n",
        "- Pearson: Linear relationship.\n",
        "- Kendall & Spearman: Non-linear monotonic relationships.\n",
        "\n",
        "\n",
        "Q15. What is causation? Explain difference between correlation and causation with an example.\n",
        "- Causation (or causality) means that one variable directly affects or causes a change in another.\n",
        "- In simple terms : If X causes Y, then changing X will change Y.\n",
        "- Difference :\n",
        "1. Definition :\n",
        "- Correlation : A statistical relationship that shows how two variables move together (increase/decrease). It tells \"what is happening together.\"\n",
        "- Causation : A cause-effect relationship where one variable directly influences or brings change in the other. It tells \"why something is happening.\"\n",
        "2. Relationship Type :\n",
        "- Correlation : The relationship is associative, not directional. Just because X and Y move together doesn’t mean one causes the other.\n",
        "- Causation : The relationship is directional. If X causes Y, then changes in X will lead to changes in Y.\n",
        "3. Does One Variable Affect the Other? :\n",
        "- Correlation : No. It only shows a pattern — not cause and effect.\n",
        "- Causation : Yes. One variable directly affects the other.\n",
        "4. Third Variable Influence :\n",
        "- Correlation : Easily affected by lurking or confounding variables that may be causing the observed relationship.\n",
        "- Causation : In well-designed studies, confounding variables are controlled or eliminated to isolate cause-effect.\n",
        "5. How It’s Measured or Proven :\n",
        "- Correlation : Measured using statistical tools like Pearson, Spearman, or Kendall correlation coefficients.\n",
        "- Causation : Proven using controlled experiments, longitudinal studies, or causal inference models (e.g., randomized controlled trials, regression with control variables).\n",
        "6. Use in Decision-Making :\n",
        "- Correlation : Helps to find patterns and generate hypotheses. Useful for exploratory analysis, but not for making strong predictions.\n",
        "- Causation : Useful for policy, science, and business decisions because it tells you what will happen if you take a specific action.\n",
        "7. Example :\n",
        "- Correlation : Ice cream sales and drowning incidents both rise in summer — they are correlated, but one doesn’t cause the other.\n",
        "- Causation : Taking a specific medicine reduces fever — the medicine causes the change.\n",
        "\n",
        "Q16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "- In Machine Learning (especially in deep learning), an optimizer is an algorithm that adjusts the model parameters (weights and biases) to minimize the loss function during training.The goal of an optimizer is to find the best model parameters that reduce prediction errors as much as possible.\n",
        "- Why Optimizers Are Important :\n",
        "  - They control the learning process.\n",
        "  - Help models converge faster.\n",
        "  - Impact the accuracy and performance of the model.\n",
        "  - Avoid getting stuck in local minima.\n",
        "- Types of Optimizers : Here are the most common types of optimizers used in deep learning:\n",
        "1. Gradient Descent (GD)\n",
        "- Description : Calculates the gradient using the entire dataset and updates weights.Simple but computationally expensive for large datasets.\n",
        "- Example :\n",
        "   - #Not commonly used directly due to large dataset inefficiency\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "- Description : Updates weights using one training example at a time.Much faster, but may fluctuate during training.\n",
        "- Example :\n",
        "   - import torch\n",
        "   - optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "3. Mini-Batch Gradient Descent\n",
        "- Description : A compromise between GD and SGD.Uses small batches of data to compute the gradient.\n",
        "Example : Used automatically in training loops with batches.\n",
        "4. Momentum\n",
        "- Description : Adds memory of past gradients to smooth out updates.Helps to accelerate training and reduce oscillations.\n",
        "- Example:\n",
        "    - optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "5. Adagrad\n",
        "- Description : Adapts learning rate individually for each parameter.Good for sparse data but learning rate may get too small.\n",
        "- Example:\n",
        "    - optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
        "6. RMSprop\n",
        "- Description : Fixes Adagrad's problem by using a moving average of squared gradients.Works well for recurrent neural networks.\n",
        "- Example:\n",
        "    - optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
        "7. Adam (Adaptive Moment Estimation)\n",
        "- Description : Combines the advantages of Momentum and RMSprop.Uses adaptive learning rates and momentum.Most commonly used optimizer.\n",
        "- Example :\n",
        "    - optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "8. AdamW\n",
        "- Description : Variation of Adam with proper weight decay for regularization.Better for deep networks like transformers.\n",
        "- Example:\n",
        "   - optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "Q17.What is sklearn.linear_model ?\n",
        "- sklearn.linear_model is a module in Scikit-learn (sklearn) that contains a collection of linear models used for both regression and classification tasks.\n",
        "- Purpose : It provides machine learning algorithms that model relationships between variables using linear equations.\n",
        "- These models aim to : Predict numeric values (regression), Classify data into categories (classification)\n",
        "1. Linear Regression\n",
        "    - from sklearn.linear_model import LinearRegression\n",
        "    - model = LinearRegression()\n",
        "    - model.fit(X_train, y_train)\n",
        "    - predictions = model.predict(X_test)\n",
        "2. Logistic Regression\n",
        "    - from sklearn.linear_model import LogisticRegression\n",
        "    - model = LogisticRegression()\n",
        "    - model.fit(X_train, y_train)\n",
        "    - predictions = model.predict(X_test)\n",
        "\n",
        "\n",
        "Q18.What does model.fit() do? What arguments must be given?\n",
        "- The method model.fit() is used to train a machine learning model on your dataset.\n",
        "- What It Does : It takes input data (features) and target labels (output) and uses them to learn the relationship between them.It adjusts the internal parameters (like weights) of the model based on the data.After fitting, the model is ready to make predictions using .predict().\n",
        "- Syntax :\n",
        "   - model.fit(X, y)\n",
        "\n",
        "Q19. What does model.predict() do? What arguments must be given?\n",
        "- The method model.predict() is used to make predictions using a model that has already been trained with .fit().\n",
        "- What It Does : Takes new input data (features) and returns the model's predicted outputs (labels or values).It uses the internal parameters (learned during fit()) to calculate predictions.\n",
        "- Syntax :\n",
        "  - model.predict(X)\n",
        "- Required Argument  : Argument\tDescription\n",
        "   - X\tFeature/input data for prediction. Should be a 2D array-like structure of shape [n_samples, n_features].\n",
        "- Example:\n",
        "   - from sklearn.linear_model import LinearRegression\n",
        "   - #Training data\n",
        "   - X_train = [[1], [2], [3], [4]]\n",
        "   - y_train = [2, 4, 6, 8]\n",
        "   - #Train the model\n",
        "   - model = LinearRegression()\n",
        "   - model.fit(X_train, y_train)\n",
        "   - #New data for prediction\n",
        "   - X_test = [[5], [6]]\n",
        "   - predictions = model.predict(X_test)\n",
        "   - print(predictions)  #Output might be [10.0, 12.0]\n",
        "\n",
        "Q20.What are continuous and categorical variables?\n",
        "1. Continuous Variables\n",
        "- Definition : A continuous variable can take any value within a range, including decimals or fractions.\n",
        "- Characteristics : Infinite possible values, Measurable, not countable, Usually represented with real numbers\n",
        "- Examples:\n",
        "   - Height (e.g., 165.3 cm)\n",
        "   - Weight (e.g., 62.5 kg)\n",
        "   - Temperature (e.g., 37.2°C)\n",
        "   - Time (e.g., 2.75 hours)\n",
        "   - Salary (e.g., ₹55,000.50)\n",
        "2. Categorical Variables\n",
        "Definition : A categorical variable represents distinct groups or categories. The values are labels or names.\n",
        "- Characteristics : Finite number of possible values, Can be nominal (no order) or ordinal (ordered), Often represented as strings or encoded numbers\n",
        "- Examples :\n",
        "  - Gender (Male, Female, Other)\n",
        "  - Blood Type (A, B, AB, O)\n",
        "  - Education Level (High School, Bachelor's, Master's)\n",
        "  - Color (Red, Green, Blue)\n",
        "  - Yes/No responses\n",
        "\n",
        "Q21.What is feature scaling? How does it help in Machine Learning?\n",
        "- Feature Scaling is a technique used to normalize or standardize the range of independent variables (features) in your dataset.\n",
        "- Why It Matters : In Machine Learning, features can have very different units and scales. For example:\n",
        "    - Age (0–100)\n",
        "    - Salary (in thousands or lakhs)\n",
        "    - Height (in cm)\n",
        "    - Exam scores (0–10)\n",
        "- If not scaled, features with larger ranges can dominate others and negatively affect the model’s performance, especially in distance-based algorithms.\n",
        "- How Feature Scaling Helps:\n",
        "- Benefit............................................................Description\n",
        "  - Speeds up convergence...............Especially important for gradient-based algorithms like linear regression, logistic regression, and neural networks.\n",
        "  - Improves model accuracy.............Makes features comparable, improving the performance of models like KNN, SVM, and K-means.\n",
        "  - Avoids dominance.....................Prevents larger-scale features from\n",
        "                                         overpowering smaller-scale ones.\n",
        "  - Maintains consistency..........Ensures all features contribute equally\n",
        "                                during training.\n",
        "\n",
        "\n",
        "Q22. How do we perform scaling in Python?\n",
        "- Feature scaling in Python is typically done using Scikit-learn (sklearn.preprocessing), which provides several standard scalers.\n",
        "- Step-by-Step Using Scikit-learn : Let’s go through the most common scaling methods with code examples:\n",
        "1. Min-Max Scaling (Normalization) : Scales features to a range of [0, 1].\n",
        "   - from sklearn.preprocessing import MinMaxScaler\n",
        "   - #Sample data\n",
        "   - import numpy as np\n",
        "   - X = np.array([[10, 200], [20, 400], [30, 600]])\n",
        "   - #Apply Min-Max Scaling\n",
        "   - scaler = MinMaxScaler()\n",
        "   - X_scaled = scaler.fit_transform(X)\n",
        "   - print(X_scaled)\n",
        "2. Standardization (Z-score Scaling) : Centers data around mean = 0 and std = 1.\n",
        "   - from sklearn.preprocessing import StandardScaler\n",
        "   - scaler = StandardScaler()\n",
        "   - X_scaled = scaler.fit_transform(X)\n",
        "   - print(X_scaled)\n",
        "3. Robust Scaling : Uses median and IQR, good for outliers.\n",
        "   - from sklearn.preprocessing import RobustScaler\n",
        "   - scaler = RobustScaler()\n",
        "   - X_scaled = scaler.fit_transform(X)\n",
        "   - print(X_scaled)\n",
        "4. MaxAbs Scaling : Scales data to the [-1, 1] range without shifting/centering.\n",
        "   - from sklearn.preprocessing import MaxAbsScaler\n",
        "   - scaler = MaxAbsScaler()\n",
        "   - X_scaled = scaler.fit_transform(X)\n",
        "   - print(X_scaled)\n",
        "\n",
        "Q23.What is sklearn.preprocessing\n",
        "- sklearn.preprocessing is a module in Scikit-learn that provides a wide range of data preprocessing techniques.\n",
        "- Purpose :It contains tools to :\n",
        "   - Scale features (normalize, standardize)\n",
        "   - Encode categorical variables\n",
        "   - Handle missing values\n",
        "   - Transform data to make it suitable for machine learning models\n",
        "- Why Preprocessing Matters :\n",
        "   - Raw data often contains:\n",
        "   - Different scales (e.g., height in cm, salary in ₹)\n",
        "   - Categorical values (e.g., gender = male/female)\n",
        "   - Missing values\n",
        "- sklearn.preprocessing helps clean and transform such data so that algorithms can work with it efficiently.\n",
        "\n",
        "Q24.How do we split data for model fitting (training and testing) in Python?\n",
        "- In Machine Learning, we split data into :\n",
        "  - Training set → used to train (fit) the model\n",
        "  - Test set → used to evaluate model performance on unseen data\n",
        "- We use:\n",
        "  - from sklearn.model_selection import train_test_split\n",
        "- Syntax :\n",
        "  - X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "- Example:\n",
        "  - from sklearn.model_selection import train_test_split\n",
        "  - #Example data\n",
        "  - X = [[1], [2], [3], [4], [5]]\n",
        "  - y = [1, 2, 3, 4, 5]\n",
        "  - #Split 80% train, 20% test\n",
        "  - X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        "  - )\n",
        "  - print(\"Train:\", X_train, y_train)\n",
        "  - print(\"Test:\", X_test, y_test)\n",
        "\n",
        "Q25.Explain data encoding?\n",
        "- What is Data Encoding in Machine Learning? : Data encoding is the process of transforming categorical (non-numeric) data into numerical format, so that machine learning models can understand and process it.Most ML algorithms require numeric inputs — they can't directly process text or categories like \"Male\" or \"Red\".\n",
        "- Why Encoding Is Important : Many real-world datasets contain categorical features (like gender, color, city, education).Encoding allows models to interpret these variables as numerical patterns.\n",
        "- Types of Encoding:\n",
        "1. Label Encoding : Converts each unique category into a unique integer.Simple and useful for ordinal data.\n",
        "2. One-Hot Encoding : Converts categories into binary (0/1) columns — one for each category.Good for nominal data (unordered).\n",
        "3. Ordinal Encoding : Similar to Label Encoding, but used when the order of categories matters.\n",
        "4. Binary Encoding, Target Encoding, Frequency Encoding (Advanced) : Used in large datasets or high-cardinality categorical features (e.g., hundreds of city names).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I0CINufdRuuD"
      }
    }
  ]
}